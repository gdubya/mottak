#!/usr/bin/env python3
import os                               # for getenv
import sys
import json
import psycopg2
import psycopg2.extras
import string
import subprocess
# For python 3.6
from subprocess import PIPE
import random
import re  # for parsing the DBSTRING

import logging

from dotenv import load_dotenv
load_dotenv()

# Todo:
# - validate filename (should be $uuid.tar)
# - check that the uploader URL has not been tampered with


# Allow for override (for dev purposes)
WORKFLOW = os.getenv('WORKFLOW', default='/srv/argo-mottak-DAG.yaml')

# Return codes.
UUIDERROR = 1  # invalid UUID
DBERROR = 10
JSONERROR = 11
IOERROR = 12
USAGEERROR = 13
ARGOERROR = 14
UNKNOWNUUID = 15
UNKNOWNIID = 16


# There needs to be a envir variable called DBSTRING
# it is on the following format:

# 'pgsql:host=10.0.0.0;dbname=foo;user=myuser;password=verydull'


def create_db_access(dbstring):
    """Create a psycopg2 compatible object from the connection string.
    The string is from PHP and we reuse it here
    """
    mystr = dbstring[6:]
    mystr = mystr.rstrip()
    d = dict(re.findall(r'(\w+)=([^;]+);?', mystr))
    # Validate dbstring:
    for key in ['user', 'password', 'host', 'dbname']:
        if key not in d.keys():
            logging.error('%s not found in DBSTRING' % key)
            exit(DBERROR)
    return d


def my_connect(conn_info):
    try:
        connection = psycopg2.connect(user=conn_info['user'],
                                      host=conn_info['host'],
                                      dbname=conn_info['dbname'],
                                      password=conn_info['password'],
                                      sslmode='require',
                                      sslrootcert='BaltimoreCyberTrustRoot.crt.pem',
                                      connect_timeout=10)

    except (Exception, psycopg2.Error) as error:
        logging.error(f"Error while connecting to PostgreSQL: {error}")
        exit(DBERROR)
    finally:
        return connection


def my_disconnect(conn):
    conn.close()


def read_tusd_event(step):
    try:
        data = json.load(sys.stdin)
    except ValueError as e:
        logging.error(f"Error parsing JSON({step}): {e}")
        exit(JSONERROR)
    # Enable this when debugging the events. It dumps the input to /tmp so you can re-run the hook with stdin.
    with open(f'/tmp/json-event-{step}.json', 'w') as event_file:
        json.dump(data, event_file)

    return data


def get_metadata(conn, iid):
    try:
        dict_cursor = conn.cursor(
            cursor_factory=psycopg2.extras.RealDictCursor)
        dict_cursor.execute('SELECT invitations.id, uuid, checksum, is_sensitive, name, email, type '
                            'FROM invitations, archive_types '
                            'WHERE archive_type_id=archive_types.id '
                            'AND invitations.id=%s', (iid,))
        rec = dict_cursor.fetchall()
    except psycopg2.Error as e:
        logging.error(f'SQL Query error: {e}')
        exit(DBERROR)

    if len(rec) == 0:
        return None
    else:
        return rec[0]


def update_db_with_objectname(conn, iid, objectname):
    cur = conn.cursor()
    cur.execute("UPDATE invitations SET object_name = %s WHERE id = %s", (objectname, iid))
    if cur.rowcount != 1:
        raise('Invalid number of rows updated')



def randomString(string_length=32):
    """Generate a random string of fixed length """
    letters = string.ascii_lowercase
    return ''.join(random.choice(letters) for i in range(string_length))


def create_param_file(tmpfile, metadata, data):
    """ Create a metadata JSON-file for Argo to ingest.
        This file contains the workflow parameters referenced in the workflow.
    
        Parameters:
        -----------
         tmpfile : str
         metadata : dict
         data : dict (data object given from tusd)
     """
    # define en workflow parameters
    logging.debug('Creating PARAM file for ARGO')
    with open(tmpfile, "w") as pfile:
        print('UUID:', metadata['uuid'], file=pfile)
        print('OBJECT:', data['Upload']['Storage']['Key'], file=pfile)
        print('CHECKSUM:', metadata['checksum'], file=pfile)
        print('ARCHIEVE_TYPE:', metadata['type'], file=pfile)
        print('NAME:', metadata['name'],  file=pfile)
        print('EMAIL:', metadata['email'], file=pfile)
        print('INVITATIONID:', metadata['id'], file=pfile)


def argo_submit(paramfile):
    """ Submit a job to argo. Takes a YAML file as parameter """
    argocmd = ["argo", "submit", "--parameter-file", paramfile, WORKFLOW]
    logging.info(f"Argo cmd line: {argocmd}")

    submit = subprocess.run(argocmd, timeout=20, stdout=PIPE, stderr=PIPE)
    # submit = subprocess.run("ls", timeout=20, stdout=PIPE, stderr=PIPE)   # stupid debugging hack to avoid spamming argo.
    if not (submit.returncode == 0):
        logging.error("Argo submit failed")
        if submit.stderr:
            logging.error(f"Stderr: {submit.stderr.decode('utf-8')}")
        if submit.stdout:
            logging.error(f"Stdout: {submit.stdout.decode('utf-8')}")
        exit(ARGOERROR)


########################################################
############# Run from here  ###########################
########################################################

# We use the same source for both pre-create and post-finish hook
# This identifies it
my_name = os.path.basename(__file__)

data = read_tusd_event(step=my_name)
param_file = '/tmp/argo-input-' + randomString()

if (os.getenv('DEBUG')):
    logging.basicConfig(level='DEBUG')

if not (os.getenv('DBSTRING')):
    logging.error("DBSTRING environment variable not set")
    exit(USAGEERROR)

try:
    iid = data["Upload"]["MetaData"]["invitation_id"]
    logging.info(f"Invitation ID from JSON: {iid}")
except:
    logging.error(f"Could not find invitation_id in JSON: {iid}")
    exit(UNKNOWNIID)


connection = my_connect(create_db_access(os.getenv('DBSTRING')))

metadata = get_metadata(connection, iid)
if (metadata == None):
    logging.error(f"Failed to fetch metadata for invitation {iid} - no invitation?")
    exit(UNKNOWNIID)

uuid = metadata['uuid']


# This is the pre-create hook. The only concern here is to validate the UUID
if (my_name == 'pre-create'):
    if not (uuid == metadata['uuid']):
        logging.error('Unknown UUID')
        exit(UUIDERROR)
    else:
        exit(0)

# We assume that we're the post-create hook and we create an input-file for argo
# and submit the workflow into argo.

# Verify that we have a filename:
try:
    filename = data['Upload']['Storage']['Key']
    logging.debug(f"File name (in objectstore) is {filename}")
except:
    logging.error("Could not key/filename in JSON. Dumping JSON:")
    logging.error(json.dump(data))
    exit(JSONERROR)

try:
    update_db_with_objectname(connection, iid, filename);
    logging.debug(f"Set object_name to {filename} for iid {iid} in database.")
except Exception as e:
    logging.error("Error while updating database {e}")
    exit(DBERRROR)

if ((metadata) and ('uuid' in metadata)):
    create_param_file(param_file, metadata, data)
    argo_submit(param_file)
    os.remove(param_file)
    my_disconnect(connection)

    exit(0)
else:
    logging.error("Unknown UUID:" + uuid)
    exit(UUIDERROR)


# Stick these things as k8s secrets and dump them into the input file.
# Then kick off argo.


# ENDPOINT: https://storage.googleapis.com
# AWS_ACCESS_KEY_ID:
# AWS_SECRET_ACCESS_KEY:
# BUCKET: pilot-spool
# OBJECT: 'uuid.tar'
# CHECKSUM: 2afeec307b0573339b3292e27e7971b5b040a5d7e8f7432339cae2fcd0eb936a
# REGION_NAME: us-east-1
# ARCHIEVE_TYPE: noark5
